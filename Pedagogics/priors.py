"""
This script was written for the ESF-financed project Demokratisk
Digitalisering. The project is a collaboration between
Arbetsförmedlingen (AF) and Kungliga Tekniska Högskolan (KTH). The
purpose of the project is to increase the digital competence - as
defined at https://ec.europa.eu/jrc/en/digcomp - of people working at
Arbetsförmedlingen and by extension also job seekers. This will be
done by learning modules at KTH using OLI-Torus.

--- About this Python script ---

This script implements a toy model that illustrates a common
misconception in statistical analysis. Frequentists often believe that
the prior used in Bayesian analyses introduces a subjective element and
that frequentist analyses permit the data to 'speak for itself'. Many
Bayesians in turn believe that they can avoid making subjective
assumptions by choosing a flat prior. This toy model demonstrates that
both these beliefs are false, by showing an analysis where
statisticians can start out fitting the same model to the same data,
and still end up with different results. This happens whether we work
in a Bayesian or a frequentist framework. The only real difference is
that Bayesianism makes it slightly easier to understand what happens.

In the toy model, we assume that there is a line segment of unit length
that extends from the origin. n measurements are generated by selecting
points along this line with uniform probability, and then adding 
gaussian noise. We then simulate statisticians trying to estimate
position of the line to the data. The statisticians can use a
frequentist approach or a Bayesian approach with a flat prior, but as
we shall see it makes no difference. They can also choose to describe
the line in terms of the angle alpha w.r.t. the x-axis, or the
coefficient a in the equation y = ax. This *should* make no difference,
since both parametrisations describe exactly the same line. But as we
shall see, it actually does make a difference to the results.

It turns out that in a Bayesian framework there is no uniquely defined
"flat prior". A prior can at most be flat w.r.t. some given
parametrisation of the underlying problem. This means that even a flat
prior encodes assumptions. Working in a frequentist framework does not
avoid this problem. The maximum-likelihood fit used in frequentist
parameter fitting is equivalent to a Bayesian fit that uses a flat
prior and throws away all but the maximum of the posterior.

Written by Alvin Gavel
https://github.com/Alvin-Gavel/Demodigi
"""

import numpy as np
import numpy.random as rd
import matplotlib.pyplot as plt



class experiment:
   """
   This represents the experiment and the following fitting of a model to
   the observed data.
   
   Attributes
   ----------
   alpha : float
   \tThe angle of the line with respect to the x-axis.
   n : int
   \tThe number of measurements taken
   sigma : float
   \tThe standard deviation in the random scatter. Note that for
   \tsimplicity the analysis assumes that this is known.
   plot_folder : str
   \tPath to the folder where all plots should be placed.
   """
   def __init__(self, alpha, n, sigma, plot_folder = 'priors_plots'):
      self.alpha = alpha
      self.n = n
      self.sigma = sigma
      self.plot_folder = plot_folder
      self.measurements = self.generate_data()
      return
      
   def generate_data(self):
      r = rd.uniform(low = 0., high = 1., size = self.n)
      x = r * np.cos(self.alpha) + rd.normal(loc=0.0, scale=self.sigma, size=self.n)
      y = r * np.sin(self.alpha) + rd.normal(loc=0.0, scale=self.sigma, size=self.n)
      return np.asarray(list(zip(x, y)))
      
   def plot_data(self):
      plt.clf()
      plt.tight_layout()
      plt.scatter(self.measurements[:,0], self.measurements[:,1], s=1, marker = 's')
      plt.plot([0, np.cos(self.alpha)], [0, np.sin(self.alpha)], c = 'k', linestyle = '--', label = 'True')
      plt.xlim(-1, 1)
      plt.ylim(-1, 1)
      plt.legend()
      plt.savefig('./{}/Measurements.png'.format(self.plot_folder))
      return
